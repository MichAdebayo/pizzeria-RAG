# ğŸ• Pizzeria RAG - Assistant de Recherche Multi-Documents

<div align="center">
  <img src="assets/pizza_asset.webp" alt="Pizzeria RAG System" width="400">
</div>

Un systÃ¨me RAG (Retrieval-Augmented Generation) moderne pour interroger plusieurs menus de pizzerias avec Ollama et ChromaDB.

## ğŸš€ FonctionnalitÃ©s

- **RAG Multi-Documents**: Recherche simultanÃ©e dans plusieurs menus de pizzerias
- **Interface Moderne**: Chainlit et Gradio pour une expÃ©rience utilisateur optimale
- **Local-First**: Utilise Ollama pour les embeddings et LLM (pas de dÃ©pendance cloud)
- **Architecture Modulaire**: Code propre, maintenable et extensible
- **Processing Intelligent**: Extraction et indexation automatique des PDFs

## ğŸ› ï¸ Architecture

```
pizzeria-RAG/
â”œâ”€â”€ assets/                 # Images et ressources
â”‚   â””â”€â”€ pizza_asset.webp
â”œâ”€â”€ config/                 # Configuration centralisÃ©e
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ data/                   # DonnÃ©es traitÃ©es et base vectorielle
â”‚   â”œâ”€â”€ processed/         # JSONs structurÃ©s des documents
â”‚   â””â”€â”€ vector_db/         # Base vectorielle ChromaDB (git-ignorÃ©)
â”œâ”€â”€ docs/                   # Documentation et sources (git-ignorÃ©)
â”‚   â””â”€â”€ raw_pdfs/          # Documents PDF sources
â”œâ”€â”€ logs/                   # Fichiers de logs (git-ignorÃ©)
â”œâ”€â”€ processors/             # Traitement des documents
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ document_processor.py
â”œâ”€â”€ src/                    # Code source principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ apps/              # Applications utilisateur
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chainlit_app.py    # Interface chat moderne
â”‚   â”‚   â””â”€â”€ gradio_app.py      # Interface web alternative
â”‚   â”œâ”€â”€ core/              # FonctionnalitÃ©s principales
â”‚   â”‚   â”œâ”€â”€ pipeline.py        # Pipeline principal
â”‚   â”‚   â”œâ”€â”€ rag_engine.py      # Moteur RAG complet
â”‚   â”‚   â””â”€â”€ vector_store.py    # Gestion embeddings & ChromaDB
â”‚   â””â”€â”€ extractors/        # Extracteurs de contenu
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_extractor.py
â”‚       â”œâ”€â”€ ocr_extractor.py
â”‚       â”œâ”€â”€ pdf_extractor.py
â”‚       â”œâ”€â”€ recipe_extractor.py
â”‚       â”œâ”€â”€ table_extractor.py
â”‚       â””â”€â”€ text_extractor.py
â”œâ”€â”€ chainlit.md             # Configuration Chainlit
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ start_chainlit.sh       # Script de lancement Chainlit
â”œâ”€â”€ start_gradio.sh         # Script de lancement Gradio
â”œâ”€â”€ .gitignore             # Fichiers ignorÃ©s par Git
â””â”€â”€ README.md              # Documentation du projet
```

## ğŸ“‹ PrÃ©requis

1. **Ollama** installÃ© et configurÃ©
2. **Python 3.9+**
3. **ModÃ¨les Ollama**:
   - `llama3.2:latest` (chat)
   - `mxbai-embed-large` (embeddings)

## ğŸ”§ Installation

1. **Cloner le projet**:
```bash
git clone <repository-url>
cd pizzeria-RAG
```

2. **CrÃ©er l'environnement virtuel**:
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou .venv\Scripts\activate  # Windows
```

3. **Installer les dÃ©pendances**:
```bash
pip install -r requirements.txt
```

4. **DÃ©marrer Ollama** (dans un terminal sÃ©parÃ©):
```bash
ollama serve
```

5. **TÃ©lÃ©charger les modÃ¨les**:
```bash
ollama pull llama3.2:latest
ollama pull mxbai-embed-large
```

## ğŸ¯ Utilisation

### Interface Chainlit (RecommandÃ©e)
```bash
./start_chainlit.sh
```
**AccÃ¨s:** http://localhost:8000

**FonctionnalitÃ©s:**
- Chat interactif moderne
- Commandes systÃ¨me intÃ©grÃ©es (`/status`, `/documents`, `/process`, `/help`)
- Affichage en temps rÃ©el du statut du systÃ¨me
- Interface Ã©purÃ©e sans informations techniques parasites

### Interface Gradio (Alternative)
```bash
./start_gradio.sh
```
**AccÃ¨s:** http://localhost:7860

**FonctionnalitÃ©s:**
- Interface web simple et intuitive
- Chat avec avatars utilisateur/assistant
- Onglet d'aide intÃ©grÃ©
- Parfait pour les dÃ©mos et tests rapides

### Utilisation directe en Python
```python
from src.core.rag_engine import LLMInterface

llm = LLMInterface()
result = llm.answer_question("Quelles pizzas vÃ©gÃ©tariennes avez-vous?")
print(result['answer'])
```

## ğŸ’¬ Exemples de Questions

- **GÃ©nÃ©rales**: "Quelles pizzas avez-vous?"
- **SpÃ©cifiques**: "Chez Anchor Pizza, quel est le prix de la Margherita?"
- **Comparatives**: "Comparez les prix entre Marco Fuso et Anchor Pizza"
- **DÃ©taillÃ©es**: "Quels sont les ingrÃ©dients du Triomphe vÃ©gÃ©?"

## ğŸ”§ Commandes SystÃ¨me (Chainlit)

- `/status` - Statut dÃ©taillÃ© du systÃ¨me
- `/documents` - Liste des documents disponibles
- `/process` - Traiter/retraiter les documents
- `/help` - Aide complÃ¨te

## ğŸ“ DonnÃ©es

Le systÃ¨me traite automatiquement les PDFs dans `docs/raw_pdfs/`:
- **Anchor Pizza**: Menu et services
- **Marco Fuso**: Recettes et techniques

Les donnÃ©es traitÃ©es sont stockÃ©es dans:
- `data/processed/` - JSONs structurÃ©s
- `data/vector_db/` - Base vectorielle ChromaDB

## âš™ï¸ Configuration

Modifiez `config/config.py` pour ajuster:
- ModÃ¨les Ollama utilisÃ©s
- ParamÃ¨tres de chunking
- TempÃ©rature du LLM
- Ports et endpoints

### SÃ©curitÃ© et Git:
- **Dossiers protÃ©gÃ©s**: `data/`, `docs/`, `logs/` exclus du versioning
- **Base vectorielle**: ChromaDB stockÃ©e localement et non exposÃ©e
- **Fichiers sensibles**: `.env`, logs et caches automatiquement ignorÃ©s
- **Historique propre**: Suppression complÃ¨te des fichiers sensibles du Git

## ğŸ› DÃ©pannage

### ProblÃ¨mes courants:

1. **Ollama non connectÃ©**:
   ```bash
   ollama serve
   curl http://localhost:11434/api/tags
   ```

2. **ModÃ¨les manquants**:
   ```bash
   ollama list
   ollama pull llama3.2:latest
   ollama pull mxbai-embed-large
   ```

3. **Scripts de lancement**:
   ```bash
   # Rendre les scripts exÃ©cutables
   chmod +x start_chainlit.sh start_gradio.sh
   
   # VÃ©rifier l'environnement virtuel
   source .venv/bin/activate
   ```

4. **Documents non indexÃ©s**:
   - Utilisez `/process` dans Chainlit
   - Ou vÃ©rifiez `data/vector_db/`

5. **ProblÃ¨mes de performance**:
   - RÃ©duisez `chunk_size` dans la config
   - VÃ©rifiez la RAM disponible

6. **Erreurs Gradio**:
   ```bash
   # Mise Ã  jour Gradio si nÃ©cessaire
   pip install --upgrade gradio
   ```

## ğŸš€ DÃ©veloppement

### AmÃ©liorations rÃ©centes:
- **Interface Chainlit Ã©purÃ©e**: SystÃ¨me de statut dÃ©placÃ© vers les commandes systÃ¨me
- **RÃ©ponses utilisateur propres**: Suppression des informations techniques des rÃ©ponses
- **Architecture refactorisÃ©e**: Organisation claire du code en modules
- **Gestion des avatars**: Interfaces utilisateur modernes avec avatars
- **Scripts de lancement**: DÃ©marrage simplifiÃ© avec vÃ©rifications automatiques

### Structure du code:
- **Modulaire**: Chaque composant a sa responsabilitÃ©
- **Type Hints**: Code typÃ© pour une meilleure maintenance
- **Logging**: TraÃ§abilitÃ© complÃ¨te des opÃ©rations
- **Async/Await**: Interface non-bloquante pour une meilleure UX
- **Gestion d'erreurs**: Robustesse et rÃ©cupÃ©ration automatique

### Ajouter de nouveaux documents:
1. Placer le PDF dans `docs/raw_pdfs/`
2. Ajouter la configuration dans `config/config.py`
3. Relancer le traitement via `/process` (Chainlit) ou l'interface Gradio

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir `LICENSE` pour plus de dÃ©tails.

## ğŸ¤ Contribution

Les contributions sont bienvenues! Merci de:
1. Fork le projet
2. CrÃ©er une branche feature
3. Commit vos changements
4. Pousser vers la branche
5. Ouvrir une Pull Request

## ğŸ“ Support

En cas de problÃ¨me:
1. VÃ©rifiez la section DÃ©pannage
2. Consultez les logs de l'application
3. Ouvrez une issue sur GitHub

---

**DÃ©veloppÃ© avec â¤ï¸ pour une expÃ©rience RAG moderne et efficace**
